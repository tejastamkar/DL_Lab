{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo5PgVOx8AJy",
        "outputId": "0463bd11-d4c5-4de1-81f2-b89de887a5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 19ms/step - accuracy: 0.7301 - loss: 0.5101 - val_accuracy: 0.8663 - val_loss: 0.3173\n",
            "Epoch 2/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9021 - loss: 0.2555 - val_accuracy: 0.8662 - val_loss: 0.3194\n",
            "Epoch 3/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9309 - loss: 0.1793 - val_accuracy: 0.8671 - val_loss: 0.3583\n",
            "Epoch 4/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.9502 - loss: 0.1376 - val_accuracy: 0.8288 - val_loss: 0.5930\n",
            "Epoch 5/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.9559 - loss: 0.1229 - val_accuracy: 0.8644 - val_loss: 0.4067\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8632 - loss: 0.4150\n",
            "Test accuracy: 0.8644400238990784\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Set parameters\n",
        "vocab_size = 10000  # Top 10,000 most frequent words\n",
        "maxlen = 200       # Maximum sequence length\n",
        "embedding_dim = 128\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(LSTM(128)) # LSTM layer with 128 units\n",
        "model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "score, accuracy = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.predict(x_test[:5]) # Predicting first 5 samples\n",
        "for i, prediction in enumerate(predictions):\n",
        "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "    print(f\"Prediction {i+1}: {prediction[0]:.4f} ({sentiment})\")\n",
        "    print(\"-\" * 35)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoIDmTVk928E",
        "outputId": "ed88575e-4ad0-4520-fbc6-bd1a71ff3cd9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
            "Prediction 1: 0.0067 (Negative)\n",
            "-----------------------------------\n",
            "Prediction 2: 0.9999 (Positive)\n",
            "-----------------------------------\n",
            "Prediction 3: 0.4894 (Negative)\n",
            "-----------------------------------\n",
            "Prediction 4: 0.2013 (Negative)\n",
            "-----------------------------------\n",
            "Prediction 5: 0.9999 (Positive)\n",
            "-----------------------------------\n"
          ]
        }
      ]
    }
  ]
}